\subsection*{Обозначения}

\begin{center}
\begin{minipage}{.9\textwidth}
	$\R$ --- поле (топологическое пространство) вещественных чисел;

	$\vec{x} = (x^1, \ldots, x^n)$ --- вектор (точка) из $\R^n$;
	
	$(\vec{e}_1, \ldots, \vec{e}_n)$ --- стандартный базис в $\R^n$;

	$\span(\vec{v}_1, \ldots, \vec{v}_n)$ --- линейная оболочка векторов $\vec{v}_1, \ldots, \vec{v}_n$;

	$S_{\Or}(\vec{u}, \vec{v})$ --- ориентированная площадь параллелограмма, натянутого на векторы $\vec{u}$ и $\vec{v}$, $\Vol_{\Or}(\vec{v}_1, \ldots, \vec{v}_n)$ --- ориентированный объём $n$-мерного параллелепипеда, натянутого на векторы $\vec{v}_1,\,\ldots,\,\vec{v}_n$;

	$I$ --- связное подмножество $\R$;

	$\Int U$ --- внутренность подмножества $U \subset \R^n$;

	$\langle\vec{x}, \vec{y}\rangle$ --- евклидово скалярное произведение векторов $\vec{x},\,\vec{y} \in \R^n$;

	$\langle\vec{x}, \vec{y}\rangle_\G$ --- скалярное произведение векторов $\vec{x},\,\vec{y} \in \R^n$, задаваемое положительно определённой симметричной матрицей $\G$ (то есть $\langle\vec{x}, \vec{y}\rangle_\G = \vec{x}^t\G\vec{y}$);

	$\vec{x} \times \vec{y}$ --- векторное произведение векторов $\vec{x},\,\vec{y} \in \R^3$;

	$\rho(\vec{x}, \vec{y})$ --- расстояние между точками $\vec{x}$ и $\vec{y}$ из $\R^n$;

	$\vec{r}(t) = (x^1(t), \ldots, x^n(t))$ --- радиус-вектор точки $\vec{x} \in \R^n$;

	$\dot{\vec{r}}(t),\,\ddot{\vec{r}}(t),\,\ldots$ --- векторы скорости, ускорения и т.\,д. точки $\vec{x} \in \R^n$.

	\medskip
	\textbf{Нотация Эйнштейна}. {\small По дважды повторяющимся индексам, один из которых верхний, а другой нижний, подразумевается суммирование в пределах, устанавливаемых из контекста, а сам такой индекс называется \textit{слепым}. Верхний индекс переменной, появляющейся в знаменателе, считается для выражения нижним, и наоборот.}
\end{minipage}
\end{center}

\section{Предварительные сведения и напоминания}

\epigraph{Сначала вы подумаете, что я сумасшедший, а потом вам понравится, и вы сами будете делать так же.}{А.\,В. Пенской}

\subsection{Математический анализ}

Отображение $\vec{f}\colon \R^n \to \R^m$ называется \textit{дифференцируемым в точке} $\vec{x}_0$, если существует линейное отображение $\mathcal{L}_{\vec{x}_0}$, для которого выполнено
\[
	\vec{f}(\vec{x}) = \vec{f}(\vec{x}_0) + \mathcal{L}_{\vec{x}_0}(\vec{x} - \vec{x}_0) + \o(\norm{\vec{x} - \vec{x}_0})\text{ при $\vec{x} \to \vec{x}_0$}.
\]

При этом отображение $\vec{f}$ не обязано быть определено всюду. Нам будет достаточно, чтобы в область определения отображения $\vec{f}$ входило замыкание некоторой выпуклой открытой области, содержащее точку $\vec{x}_0$. Однозначно определённое линейное отображение $\mathcal{L}_{\vec{x}_0} = \vcentcolon \left.d\vec{f}\right|_{\vec{x_0}}$ называют \textit{дифференциалом} отображения $\vec{f}$ в точке $\vec{x}$.

Матрица $J_{\vec{f}}(\vec{x}_0)$ линейного отображения $\left.d\vec{f}\right|_{\vec{x}_0}$ называется \textit{матрицей Якоби} отображения $\vec{f}$ в точке $\vec{x}_0$ и состоит из \textit{частных производных}:

\[
	J_{\vec{f}}(\vec{x}_0) =
	\begin{pmatrix}
		\ds\left.\frac{\partial f^1}{\partial x^1}\right|_{\vec{x}_0} & \ds\ldots & \ds\left.\frac{\partial f^1}{\partial x^n}\right|_{\vec{x}_0} \\
		\ds\vdots & \ds\ddots & \ds\vdots \\
		\ds\left.\frac{\partial f^m}{\partial x^1}\right|_{\vec{x}_0} & \ds\ldots & \ds\left.\frac{\partial f^m}{\partial x^n}\right|_{\vec{x}_0} \\
	\end{pmatrix} =
	\begin{pmatrix}
		\left.\grad f^1\right|_{\vec{x}_0} \\
		\vdots\\
		\left.\grad f^m\right|_{\vec{x}_0} \\
	\end{pmatrix}.
\]
В случае, когда эта матрица квадратная, её определитель называют \textit{якобиантом}.

Дифференцируемое отображение $\vec{f}$ определяет новое отображение $\partial \vec{f} / \partial \vec{x}\colon \R^n \to \R^m$. Если последнее также дифференцируемо, то $\vec{f}$ называется \textit{дважды дифференцируемым}, и далее индуктивно: если $\partial\vec{f} / \partial\vec{x}$ дифференцируемо $k$ раз, то $\vec{f}$ дифференцируемо $k + 1$ раз. Если отображение $\vec{f}$ дифференцируемо $k$ раз и при $k$-кратном дифференцировании получается непрерывное отображение, то говорят, что $\vec{f}$ \textit{$k$ раз непрерывно дифференцируемо} или является \textit{отображением класса $C^k$}. В дальнейшем под \textit{гладким отображением} мы будем подразумевать отображение класса $C^k$ для достаточно большого $k$.

\begin{theorem}[О производной сложной функции]
	Если отображения $\vec{f}\colon \R^n \to \R^m$ и $\vec{g}\colon \R^m \to \R^k$ дифференцируемы, то дифференцируема и композиция $\vec{g} \circ \vec{f}$, причём
	\[
		\left.d(\vec{g} \circ \vec{f})\right|_{\vec{x}_0} = \left.d\vec{g}\right|_{\vec{f}(\vec{x_0})} \circ \left.d\vec{f}\right|_{\vec{x}_0}.
	\]
\end{theorem}

\begin{theorem}[Об обратном отображении]
	Гладкое отображение $\vec{f}\colon \R^n \to \R^n$, матрица Якоби которого невырожденна в точке $\vec{x}_0$, локально обратимо в некоторой окрестности точки $\vec{x}_0$, причём обратное отображение также гладкое.
\end{theorem}

\begin{theorem}[О неявном отображении]
	Пусть $\vec{f}\colon \R^n \to \R^m$, $m \leqslant n$, --- гладкое отображение, матрица Якоби которого в точке $\vec{x}_0$ имеет ранг $m$. Тогда множество решений уравнения $\vec{f}(\vec{x}) = \vec{f}(\vec{x}_0)$ в окрестности точки $\vec{x}_0$ выглядит как график гладкого отображения, выражающего некоторые $m$ координат через оставшиеся $n - m$, причём эти $m$ координат можно выбрать те, которым соответствуют линейно независимые столбцы в матрице Якоби.
\end{theorem}

\subsection{Аналитическая геометрия и линейная алгебра}

Пусть в $\R^n$ есть некоторая поверхность, задаваемая уравнением $F(x^1, \ldots, x^n) = 0$, а по ней движется точка, радиус-вектор которой есть $\vec{x} = \vec{r}(t)$. Тогда можем продифференцировать тождество $F(r^1(t), \ldots, r^n(t)) = 0$ в каждой точке, получив по теореме о сложной функции
\[
	\frac{\partial F}{\partial r^1} \cdot \frac{d r^1}{dt} + \ldots + \frac{\partial F}{\partial r^n} \cdot \frac{d r^n}{dt} = 0
\]
или, что то же, $\langle \grad F, \dot{\vec{r}} \rangle = 0$.

Из правила Лейбинца сразу следует формула дифференцирования скалярного произведения:
\[
	\frac{d}{dt}\langle \vec{a}(t), \vec{b}(t) \rangle = \langle \dot{\vec{a}}(t), \vec{b}(t) \rangle + \langle \vec{a}(t), \dot{\vec{b}}(t) \rangle.
\]

Важный частный случай: если $\vec{a}(t) \perp \vec{b}(t)$ для всех значений параметра $t$, то $\langle\vec{a}(t), \dot{\vec{b}}(t)\rangle \hm= -\langle\dot{\vec{a}}(t), \vec{b}(t)\rangle$. Аналогичная формула верна и для векторного произведения:
\[
	\frac{d}{dt}(\vec{a}(t) \times \vec{b}(t)) = (\dot{\vec{a}}(t) \times \vec{b}(t)) + (\vec{a}(t) \times \dot{\vec{b}}(t)).
\]

Пусть $\vec{r}\colon \R \to \R^n$. Тогда $\abs{\vec{r}} = \const$ тогда и только тогда, когда $\langle \vec{r}, \dot{\vec{r}} \rangle = 0$. Доказательство простое --- надо продифференцировать тождество $\langle \vec{r}(t), \vec{r}(t) \rangle = \const$. Можно доказать и по-другому --- вектор постоянной длины $\abs{\vec{r}} = \const$ лежит на сфере, уравнение которой $F(x_1, \ldots, x_n) = x_1^2 + \ldots + x_n^2 = \const$. При этом
\[\begin{tikzcd}
	{0} & {\langle\grad{F}, \dot{\vec{r}}\rangle} & {\langle 2\vec{r}, \dot{\vec{r}}\rangle = 2\langle\vec{r}, \dot{\vec{r}}\rangle}.
	\arrow[equals, from=1-2, to=1-1]
	\arrow[equals, from=1-2, to=1-3]
\end{tikzcd}\]

Проекция вектора $\vec{u}$ на вектор $\vec{v}$ вычисляется по формуле
\[
	\proj_{\vec{v}}\vec{u} = \frac{\langle\vec{u}, \vec{v}\rangle}{\langle\vec{v}, \vec{v}\rangle} \cdot \vec{v}.
\]

Объём сведений из линейной алгебры, которые необходимы в курсе дифференциальной геометрии (и в других дисциплинах), начинает становиться слишком большим для маленького раздела напоминаний в этом файле, поэтому я решил вынести его в отдельный проект, с которым можно ознакомиться \href{https://github.com/pshenikita/Linal-Teormin}{по ссылке}.

\subsection{Дифференциальные уравнения}

\begin{theorem}[О существовании и единственности]
	Решение дифференциального уравнения $\dot{\vec{x}} = \vec{v}(\vec{x}, t)$ с начальным условием $\vec{x}(t_0) = \vec{x}_0$ из области гладкости правой части существует и единственно (в том смысле, что всякие два решения с общим начальным условием совпадают в некоторой окрестности точки $t_0$).
\end{theorem}

Помимо этой классической теоремы, нам понадобится похожее утверждение для систем дифференциальных уравнений с двумя параметрами: 
\begin{equation} \label{eq:DiffSystem}
	\begin{cases}
		\begin{aligned}
			\frac{\partial\vec{x}}{\partial u^1} = \vec{f}_1(\vec{x}, u^1, u^2),\\
			\frac{\partial\vec{x}}{\partial u^2} = \vec{f}_2(\vec{x}, u^1, u^2),
		\end{aligned}
	\end{cases}
\end{equation}
где $\vec{x} = (x^1, \ldots, x^n)$ --- неизвестная функция от $u^1$, $u^2$ со значениями в $\R^n$, а $\vec{f}_i = (\vec{f}_i^1, \ldots, \vec{f}_i^n)$, $i = 1, 2$, --- известные гладкие функции, определённые в некоторой открытой области $\Omega$ фазового пространства $\R^n \times \R \times \R \cong \R^{n + 2}$.

\begin{definition}
	Система \eqref{eq:DiffSystem} \textit{совместна}, если для любой тройки $(\vec{x}_0, u^1_0, u^2_0) \in \Omega$ она имеет гладкое решение $\vec{x}(u^1, u^2)$ с начальным условием $\vec{x}(u^1_0, u^2_0) = \vec{x}_0$, определённое в некоторой окрестности точки $(u^1_0, u^2_0)$.
\end{definition}

\begin{theorem}[Дарбу] \label{theorem:Darboux}
	Система \eqref{eq:DiffSystem} совместна тогда и только тогда, когда функции $\vec{f}_1$, $\vec{f}_2$ удовлетворяет следующему условию всюду в $\Omega$:
	\begin{equation} \label{eq:Darboux}
		\frac{\partial \vec{f}_1}{\partial x^i}f^i_2 + \frac{\partial\vec{f}_1}{\partial u^2} = \frac{\partial\vec{f}_2}{\partial x^j}f^j_1 + \frac{\partial\vec{f}_2}{\partial u^1}.
	\end{equation}
\end{theorem}

\begin{proof}
	$\Rightarrow$. Пусть $\vec{x}(u^1, u^2)$ --- гладкое решение системы с начальным условием $(u^1_0, u^2_0, \vec{x})$. Тогда по теореме о дифференцировании сложной функции имеем
	\begin{multline*}
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \frac{\partial\vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}{\partial u^2} = \frac{\partial\vec{f}_1}{\partial x^i}(\vec{x}(u^1, u^2), u^1, u^2)\frac{\partial x^i}{\partial u^2} + {}\\{} + \frac{\partial\vec{f}_1}{\partial u^2}(\vec{x}(u^1, u^2), u^1, u^2) = \br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2).
	\end{multline*}
	Отсюда
	\[
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2).
	\]
	Аналогично получаем
	\[
		\br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2) = \frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \br{\frac{\partial\vec{f}_2}{\partial x^j}f_1^j + \frac{\partial\vec{f}_2}{\partial u^1}}(\vec{x}(u^1, u^2), u^1, u^2).
	\]

	$\Leftarrow$. Можем рассмотреть первое уравнение данной системы как обыкновенное дифференциальное уравнение по переменной $u^1$ с параметром $u^2$. Из теоремы о существовании и единственности следует, что при каждом значении $u^2 = u^2_0$ существует функция $\vec{y}(u^1)$, определённая в окрестности точки $u^1_0$ такая, что
	\[
		\frac{\partial\vec{y}}{\partial u^1} = \vec{f}_1(\vec{x}, u^1, u^2_0),\quad\vec{y}(u^1_0) = \vec{x}_0.
	\]

	Теперь для каждого $u^1$, для которого определено $\vec{y}(u^1)$, мы можем решить второе уравнение с начальным условием $\vec{x}(u^1, u_0^2) = \vec{y}(u^1)$. По той же теореме о существовании и единственности, полученное решение $\vec{x}(u^1, u^2)$ будет определено для всех $u^2$ из достаточно малой, не зависящей от $u^1$, окрестности точки $u^2_0$, и будет гладкой функцией от $u^1$, $u^2$.

	Итак, в малой окрестности точки $(u^1_0, u^2_0)$ мы построили гладкую функцию $\vec{x}(u^1, u^2)$, удовлетворяющую второму уравнению всюду, а первому --- во всех точках вида $(u^1, u^2_0)$, а также удовлетворяющую начальному условию $\vec{x}(u^1_0, u^2_0) = \vec{x}_0$. Осталось показать, что $\vec{x}(u^1, u^2)$ и первому уравнению удовлетворяет всюду, а не только вдоль прямой $u^2 = u^2_0$.

	Как и выше, получаем
	\[
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\frac{\partial x^j}{\partial u^1} + \frac{\partial\vec{f}_2}{\partial u^1}(\vec{x}(u^1, u^2), u^1, u^2).
	\]
	(Только теперь мы ничего не знаем про $\partial x^j / \partial u^1$.) Используем условие \eqref{eq:Darboux}:
	\[
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \underline{\frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\frac{\partial x^j}{\partial u^1}} + \br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2} - \underline{\frac{\partial\vec{f}_2}{\partial x^j}f_1^j}}(\vec{x}(u^1, u^2), u^1, u^2).
	\]
	Далее можем сгруппировать подчёркнутые суммы в одну:
	\begin{multline*}
		\frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\br{\frac{\partial x^j}{\partial u^1} - f_1^j(\vec{x}(u^1, u^2), u^1, u^2)} = \frac{\partial^2\vec{x}}{\partial u^1\partial u^2} - {}\\{} - \br{\frac{\partial\vec{f}_1}{\partial x^i}\underline{f_2^i} + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2) = \left\{\vec{f}_2(\vec{x}(u^1, u^2), u^1, u^2) = \frac{\partial\vec{x}}{\partial u^2}\right\} =\\ = \frac{\partial^2\vec{x}}{\partial u^1\partial u^2} - {\underbrace{\br{\frac{\partial\vec{f}_1}{\partial x^i}\frac{\partial x^i}{\partial u^2} + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2)}_{\frac{\scriptstyle\partial\vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}{\scriptstyle\partial u^2}}} = \frac{\partial}{\partial u^2}\br{\frac{\partial\vec{x}}{\partial u^1} - \vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}.
	\end{multline*}
	В итоге мы получаем:
	\[
		\frac{\partial}{\partial u^2}\underbrace{\br{\frac{\partial\vec{x}}{\partial u^1} - \vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}}_{{} \vcentcolon = \vec{g}(u^1, u^2)} = \frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\underbrace{\br{\frac{\partial x^j}{\partial u^1} - f_1^j(\vec{x}(u^1, u^2), u^1, u^2)}}_{g^j(u^1, u^2)},
	\]
	это дифференциальное уравнение на функцию $\vec{g}$. Его можно рассмотреть как обыкновенное дифференциальное уравнение по $u^2$ с параметром $u^1$. При этом для всех $u^1$ выполнено начальное условие $\vec{g}(u^1, u^2_0) = \vec{0}$, откуда $\vec{g}(u^1, u^2) \equiv \vec{0}$. Это и означает, что $\vec{x}(u^1, u^2)$ всюду удовлетворяет первому уравнению системы.
\end{proof}

Отметим, что уравнение \eqref{eq:Darboux} не нужно запоминать. Оно является просто записью, очевидно, необходимого условия
\[
	\frac{\partial\vec{x}}{\partial u^1 \partial u^2} = \frac{\partial\vec{x}}{\partial u^2 \partial u^1}.
\]
Нетривиальное утверждение теоремы заключается в том, что это условие оказывается не только необходимым, но и достаточным.

\begin{example} \label{example:SimpleDiffJointness}
	Если правые части системы \eqref{eq:DiffSystem} не зависят от $\vec{x}$:
	\[
		\begin{cases}
			\begin{aligned}
				&\frac{\partial\vec{x}}{\partial u^1} = \vec{f}_1(u^1, u^2),\\
				&\frac{\partial\vec{x}}{\partial u^2} = \vec{f}_2(u^1, u^2),
			\end{aligned}
		\end{cases}
	\]
	то условие совместности для них выглядит следующим образом:
	\[
		\frac{\vec{f}_1}{\partial u^2} = \frac{\vec{f}_2}{\partial u^1}.
	\]
\end{example}

\begin{theorem}[О продолжении] \label{theorem:ContinuityDifferential}
	Решение дифференциального уравнения $\dot{\vec{x}} = \vec{v}(\vec{x}, t)$ с начальным условием из компакта в фазовом пространстве можно продолжить в обе стороны либо неограниченно, либо до границы этого компакта.
\end{theorem}

\subsection{Про функции в геометрии}

Фразу, упомянутую в эпиграфе к данному разделу, А.\,В. Пенской произнёс на первой лекции своего курса по дифференциальной геометрии и относилась она к записи вида
\[
	\vec{r}(t) = \vec{r}(s(t)).
\]

С точки зрения анализа эта запись, конечно же, некорректна, но в геометрии ей можно придать следующий смысл.

\shorthandoff{"}%
\[\begin{tikzcd}
	{\R_t} \\
	& {\mathbb{E}} && {\mathbb{R}^n} \\
	{\R_s}
	\arrow["\Phi", from=1-1, to=2-2]
	\arrow["\vec{r}", from=2-2, to=2-4]
	\arrow["\Psi"', from=3-1, to=2-2]
	\arrow["\homeo" left, "\alpha" right, from=1-1, to=3-1]
\end{tikzcd}\]
\shorthandon{"}%

Имеет место такая коммутативная диаграмма, где отображения $\Phi$ и $\Psi$ задают выбор параметра (соответственно, $t$ или $s$), а $\alpha$ --- дифферморфизм, выражающий смену параметра. Запись $\vec{r}(t) = \vec{r}(s(t))$ мы будем понимать не буквально, а следующим образом:
\[
	\vec{r} \circ \Phi = \vec{r} \circ \Psi \circ \alpha.
\]

В курсе мы будем довольно часто пользоваться такой записью, так как она чрезвычайно удобна для выражения кривой в разных параметризациях. С геометрической точки зрения нам хочется так писать, ведь кривая от смены параметра не меняется.

