\subsection*{Обозначения}

\begin{center}
	\begin{minipage}{.9\textwidth}
		$\R$ --- поле (топологическое пространство) вещественных чисел;

		$\vec{x} = (x^1, \ldots, x^n)$ --- вектор (точка) из $\R^n$;
		
		$(\vec{e}_1, \ldots, \vec{e}_n)$ --- стандартный базис в $\R^n$, если не сказано иное;

		$\span(\vec{v}_1, \ldots, \vec{v}_n)$ --- линейная оболочка векторов $\vec{v}_1, \ldots, \vec{v}_n$;

		$S_{\Or}(\vec{u}, \vec{v})$ --- ориентированная площадь параллелограмма, натянутого на векторы $\vec{u}$ и $\vec{v}$, $\Vol_{\Or}(\vec{v}_1, \ldots, \vec{v}_n)$ --- ориентированный объём $n$-мерного параллелепипеда, натянутого на векторы $\vec{v}_1,\,\ldots,\,\vec{v}_n$;

		$I$ --- связное подмножество $\R$;

		$\Int U$ --- внутренность подмножества $U \subset \R^n$;

		$\langle\vec{x}, \vec{y}\rangle$ --- евклидово скалярное произведение векторов $\vec{x},\,\vec{y} \in \R^n$;

		$\langle\vec{x}, \vec{y}\rangle_\G$ --- скалярное произведение векторов $\vec{x},\,\vec{y} \in \R^n$, задаваемое положительно определённой симметричной матрицей $\G$ (то есть $\langle\vec{x}, \vec{y}\rangle_\G = \vec{x}^t\G\vec{y}$);

		$\vec{x} \times \vec{y}$ --- векторное произведение векторов $\vec{x},\,\vec{y} \in \R^3$;

		$\rho(\vec{x}, \vec{y})$ --- расстояние между точками $\vec{x}$ и $\vec{y}$ из $\R^n$;

		$\vec{r}(t) = (x^1(t), \ldots, x^n(t))$ --- радиус-вектор точки $\vec{x} \in \R^n$;

		$\dot{\vec{r}}(t),\,\ddot{\vec{r}}(t),\,\ldots$ --- векторы скорости, ускорения и т.\,д. точки $\vec{x} \in \R^n$;

		$L_{\vec{v}}f$ --- производная функции $f$ по направлению вектора $\vec{v}$.

		\medskip
		\textbf{Тензорная нотация}. {\small По дважды повторяющимся индексам, один из которых верхний, а другой нижний, подразумевается суммирование в пределах, устанавливаемых из контекста, а сам такой индекс называется \textit{слепым}. Верхний индекс переменной, появляющейся в знаменателе, считается для выражения нижним.}
	\end{minipage}
\end{center}

\section{Предварительные сведения и напоминания}

\epigraph{Сначала вы подумаете, что я сумасшедший, а потом вам понравится, и вы сами будете делать так же.}{А.\,В. Пенской}

\subsection{Математический анализ}

Отображение $\vec{f}\colon \R^n \to \R^m$ называется \textit{дифференцируемым в точке} $\vec{x}_0$, если существует линейное отображение $\mathcal{L}_{\vec{x}_0}$, для которого выполнено
\[
	\vec{f}(\vec{x}) = \vec{f}(\vec{x}_0) + \mathcal{L}_{\vec{x}_0}(\vec{x} - \vec{x}_0) + \o(\norm{\vec{x} - \vec{x}_0})\text{ при $\vec{x} \to \vec{x}_0$}.
\]

При этом отображение $\vec{f}$ не обязано быть определено всюду. Нам будет достаточно, чтобы в область определения отображения $\vec{f}$ входило замыкание некоторой выпуклой открытой области, содержащее точку $\vec{x}_0$. Однозначно определённое линейное отображение $\mathcal{L}_{\vec{x}_0} = \vcentcolon \left.\d\vec{f}\right|_{\vec{x_0}}$ называют \textit{дифференциалом} отображения $\vec{f}$ в точке $\vec{x}$.

Матрица $J_{\vec{f}}(\vec{x}_0)$ линейного отображения $\left.\d\vec{f}\right|_{\vec{x}_0}$ называется \textit{матрицей Якоби} отображения $\vec{f}$ в точке $\vec{x}_0$ и состоит из \textit{частных производных}:

\[
	J_{\vec{f}}(\vec{x}_0) =
	\begin{pmatrix}
		\ds\left.\frac{\partial f^1}{\partial x^1}\right|_{\vec{x}_0} & \ds\ldots & \ds\left.\frac{\partial f^1}{\partial x^n}\right|_{\vec{x}_0} \\
		\ds\vdots & \ds\ddots & \ds\vdots \\
		\ds\left.\frac{\partial f^m}{\partial x^1}\right|_{\vec{x}_0} & \ds\ldots & \ds\left.\frac{\partial f^m}{\partial x^n}\right|_{\vec{x}_0} \\
	\end{pmatrix} =
	\begin{pmatrix}
		\left.\grad f^1\right|_{\vec{x}_0} \\
		\vdots\\
		\left.\grad f^m\right|_{\vec{x}_0} \\
	\end{pmatrix}.
\]
В случае, когда эта матрица квадратная, её определитель называют \textit{якобианом}.

Дифференцируемое отображение $\vec{f}$ определяет новое отображение $J_{\vec{f}}\colon \R^n \to \R^{mn}$. Если последнее также дифференцируемо, то $\vec{f}$ называется \textit{дважды дифференцируемым}, и далее индуктивно: если $\partial\vec{f} / \partial\vec{x}$ дифференцируемо $k$ раз, то $\vec{f}$ дифференцируемо $k + 1$ раз. Если отображение $\vec{f}$ дифференцируемо $k$ раз и при $k$-кратном дифференцировании получается непрерывное отображение, то говорят, что $\vec{f}$ \textit{$k$ раз непрерывно дифференцируемо} или является \textit{отображением класса $C^k$}. В дальнейшем под \textit{гладким отображением} мы будем подразумевать отображение класса $C^k$ для достаточно большого $k$.

\begin{theorem}[О производной сложной функции]
	Если отображения $\vec{f}\colon \R^n \to \R^m$ и $\vec{g}\colon \R^m \to \R^k$ дифференцируемы, то дифференцируема и композиция $\vec{g} \circ \vec{f}$, причём
	\[
		\left.\d(\vec{g} \circ \vec{f})\right|_{\vec{x}_0} = \left.\d\vec{g}\right|_{\vec{f}(\vec{x_0})} \circ \left.\d\vec{f}\right|_{\vec{x}_0}.
	\]
\end{theorem}

\begin{theorem}[Об обратном отображении]
	Гладкое отображение $\vec{f}\colon \R^n \to \R^n$, матрица Якоби которого невырожденна в точке $\vec{x}_0$, локально обратимо в некоторой окрестности точки $\vec{x}_0$, причём обратное отображение также гладкое.
\end{theorem}

Обсудим важное понятие производной функции по направлению. Пусть $\vec{v}$ --- приложенный в точке $\vec{x}$ области $U \subset \R^n$ вектор, и пусть $f\colon U \to \R$ --- дифференцируемая функция. Пусть $\vec{\varphi}\colon I \to U$ --- какая-либо регулярно параметризованная кривая (мы определим это понятие чуть позже), выходящая из $\vec{x}$ со скоростью $\vec{v}$, так что $\vec{\varphi}(0) = \vec{x}$, $\dot{\vec{\varphi}}(\vec{x}) = \vec{v}$. Возникает сквозное отображение интервала $I$ на вещественную ось, $f \circ \vec{\varphi}\colon I \to \R$.

\begin{definition}
	\textit{Производной функции $f$ по направлению вектора $\vec{v}$} называется производная построенной функции в нуле.
\end{definition}

Это число обозначается через $L_{\vec{v}}f$. Надо проверить, что полученное число не зависит от выбора кривой $\vec{\varphi}$. Это видно, например, из выражения производной по направлению через координаты: по правилу дифференцирования сложной функции
\[
	L_{\vec{v}}f = \left.\frac{\d}{\d t}(f \circ \vec{\varphi})\right|_{t = 0} = \frac{\partial f}{\partial x^i}v^i.
\]
То же самое можно выразить, сказав, что $L_{\vec{v}}f$ есть значение оператора $\d f$ на векторе $\vec{v}$.

\begin{theorem}[О неявном отображении]
	Пусть $\vec{f}\colon \R^n \to \R^m$, $m \leqslant n$, --- гладкое отображение, матрица Якоби которого в точке $\vec{x}_0$ имеет ранг $m$. Тогда множество решений уравнения $\vec{f}(\vec{x}) = \vec{f}(\vec{x}_0)$ в окрестности точки $\vec{x}_0$ выглядит как график гладкого отображения, выражающего некоторые $m$ координат через оставшиеся $n - m$, причём этих $m$ координат можно выбрать те, которым соответствуют линейно независимые столбцы в матрице Якоби.
\end{theorem}

\subsection{Аналитическая геометрия и линейная алгебра}

Пусть в $\R^n$ есть некоторая поверхность (это понятие мы тоже обсудим позднее), задаваемая уравнением $F(x^1, \ldots, x^n) = 0$, а по ней движется точка, радиус-вектор которой есть $\vec{x} = \vec{r}(t)$. Тогда можем продифференцировать тождество $F(r^1(t), \ldots, r^n(t)) = 0$ в каждой точке, получив по теореме о сложной функции
\[
	\frac{\partial F}{\partial r^1} \cdot \frac{\d r^1}{\d t} + \ldots + \frac{\partial F}{\partial r^n} \cdot \frac{\d r^n}{\d t} = 0
\]
или, что то же, $\langle \grad F, \dot{\vec{r}} \rangle = 0$. Таким образом нормаль к поверхности $F(x^1, \ldots, x^n) = 0$ в точке $\vec{x}_0$ задаётся градиентом $(\grad F)|_{\vec{x}_0}$.

Из правила Лейбинца следует формула дифференцирования скалярного произведения:
\[
	\frac{\d}{\d t}\langle \vec{a}(t), \vec{b}(t) \rangle = \langle \dot{\vec{a}}(t), \vec{b}(t) \rangle + \langle \vec{a}(t), \dot{\vec{b}}(t) \rangle.
\]

Важный частный случай: если $\vec{a}(t) \perp \vec{b}(t)$ для всех $t$, то $\langle\vec{a}(t), \dot{\vec{b}}(t)\rangle = -\langle\dot{\vec{a}}(t), \vec{b}(t)\rangle$. Аналогичная формула верна и для векторного произведения:
\[
	\frac{\d}{\d t}(\vec{a}(t) \times \vec{b}(t)) = (\dot{\vec{a}}(t) \times \vec{b}(t)) + (\vec{a}(t) \times \dot{\vec{b}}(t)).
\]

Пусть $\vec{r}\colon \R \to \R^n$. Тогда $\abs{\vec{r}} = \const$ тогда и только тогда, когда $\langle \vec{r}, \dot{\vec{r}} \rangle = 0$. Доказательство простое --- надо продифференцировать тождество $\langle \vec{r}(t), \vec{r}(t) \rangle = \const$. Можно доказать и по-другому --- вектор постоянной длины $\abs{\vec{r}} = \const$ лежит на сфере, уравнение которой $F(x_1, \ldots, x_n) = x_1^2 + \ldots + x_n^2 = \const$. При этом
\[\begin{tikzcd}
	{0} & {\langle\grad{F}, \dot{\vec{r}}\rangle} & {\langle 2\vec{r}, \dot{\vec{r}}\rangle = 2\langle\vec{r}, \dot{\vec{r}}\rangle}.
	\arrow[equals, from=1-2, to=1-1]
	\arrow[equals, from=1-2, to=1-3]
\end{tikzcd}\]

\begin{theorem}
	Для любого набора $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k$ линейно независимых векторов евклидова пространства существует, и притом единственный, ортонормированный базис $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_k$ подпространства $\span(\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k)$, порождённого этими векторами, такой, что матрица перехода $R$ от $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_k$ к $\vec{v}_1, \vec{v}_2, \ldots, \vec{v}_k$ верхнетреугольная с положительными числами на диагонали:
	\[
		\begin{pmatrix}
			\vec{v}_1 & \vec{v}_2 & \cdots & \vec{v}_k
		\end{pmatrix} =
		\begin{pmatrix}
			\vec{e}_1 & \vec{e}_2 & \cdots & \vec{e}_k
		\end{pmatrix} \cdot R.
	\]
\end{theorem}

Напомним идеи доказательства этой теоремы. Единственность базиса $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_k$ с указанными свойствами следует из того, что верхнетреугольная матрица с положительными числами на диагонали является ортогональной тогда и только тогда, когда она единичная.

Существование же базиса $\vec{e}_1, \vec{e}_2, \ldots, \vec{e}_k$ доказывается путём его явного построения, которое называется \textit{ортогонализацией Грама "---Шмидта}:
\[
	\begin{array}{r@{\;}l@{\quad}r@{\;}l}
		\vec{u}_1 = & \vec{v}_1, & \vec{e}_1 = & \vec{u}_1\,/\,|\vec{u}_1|, \\
		\vec{u}_2 = & \vec{v}_2 - \langle \vec{e}_1, \vec{v}_2 \rangle \vec{e}_1, & \vec{e}_2 = & \vec{u}_2\,/\,|\vec{u}_2|, \\
		\vec{u}_3 = & \vec{v}_3 - \langle \vec{e}_1, \vec{v}_3 \rangle \vec{e}_1 - \langle \vec{e}_2, \vec{v}_3 \rangle \vec{e}_2, & \vec{e}_3 = & \vec{u}_3\,/\,|\vec{u}_3|, \\
			& \hspace{1.7cm} \vdots & & \hspace{.2cm} \vdots \\
		\vec{u}_k = & \vec{v}_k - \sum\limits_{j=1}^{k-1} \langle \vec{e}_j, \vec{v}_k \rangle \vec{e}_j, & \vec{e}_k = & \vec{u}_k\,/\,|\vec{u}_k|
	\end{array}
\]

Также в курсе дифференциальной геометрии мы регулярно будем работать с тензорами, так что нам необходимо вспомнить соответствующие понятия и факты.

\begin{definition}
	\textit{Тензором} называется объект, задаваемый в каждой системе координат $(u^1, \ldots, u^n)$ набором чисел $T^{i_1, \ldots, i_p}_{j_1, \ldots, j_q}$, которые при замене координат $(u^1, \ldots, u^n) \mapsto (v^1, \ldots, v^n)$, преобразуются по следующему правилу:
	\[
		\widetilde{T}^{i_1, \ldots, i_p}_{j_1, \ldots, j_q} = \frac{\partial v^{i_1}}{\partial u^{k_1}} \ldots \frac{\partial v^{i_p}}{\partial u^{k_p}}\frac{\partial u^{l_1}}{\partial v^{j_1}} \ldots \frac{\partial v^{l_q}}{\partial u^{j_q}}T^{k_1, \ldots, k_p}_{l_1, \ldots, l_q},
	\]
	где $\widetilde{T}^{i_1, \ldots, i_p}_{j_1, \ldots, j_q}$ --- числовая запись тензора в координатах $(v^1, \ldots, v^n)$. Про тензор $T$ с $p$ верхними индексами и $q$ нижними говорят, что он имеет \textit{тип} $(p, q)$ и ранг $p + q$.
\end{definition}

Мы часто будем рассматривать \textit{тензоры, прикреплённые к точкам} (на поверхностях или, в более общем случае, на многообразиях), то есть определённые локально в окрестности каждой точки и зависящий от неё. Важными примерами являются метрический тензор $g_{ij}(\vec{x})$, векторные поля $V^i(\vec{x})$ или тензор кривизны Римана $R^s_{ijl}(\vec{x})$. В большинстве случаев мы не указываем явно зависимость от точки.

Легко доказать, что в любой точке $n$-мерного пространства тензоры типа $(p, q)$ образуют линейное пространство относительно операций сложения:
\[
	(T + S)^{i_1, \ldots, i_p}_{j_1, \ldots, j_q} \vcentcolon = T^{i_1, \ldots, i_p}_{j_1, \ldots, j_q} + S^{i_1, \ldots, i_p}_{j_1, \ldots, j_q}
\]
и умножения на скаляры:
\[
	(\lambda T)^{i_1, \ldots, i_p}_{j_1, \ldots, j_q} \vcentcolon = \lambda T^{i_1, \ldots, i_p}_{j_1, \ldots, j_q}.
\]
Размерность этого пространства равна $n^{p + q}$. Отметим, что складывать можно только тензоры, приклеплёные к одной и той же точке.

\begin{example}[Тензоры малых рангов]
	\begin{enumerate}[nolistsep, label=(\arabic*)]
		\item Пусть $\vec{\xi}$ --- $n$-мерный вектор, тогда при замене координат $(u^1, \ldots, u^n) \mapsto (v^1, \ldots, v^n)$ он меняется по правилу
			\[
				\widetilde{\xi}^i = \frac{\partial v^i}{\partial u^j}\xi^j,
			\]
			так что является тензором типа $(1, 0)$.
		\item Далее, пусть $f\colon \R^n \to \R$ --- гладкая функция, тогда её градиент $\vec{\eta} \vcentcolon = \grad f$ меняется по правилу
			\[
				\widetilde{\eta}_i = \frac{\partial u^j}{\partial v^i}\eta_j.
			\]
			Итак, мы получили тензор типа $(0, 1)$ --- \textit{ковектор}. Однако в текущем курсе мы иногда воспринимаем градиент функции именно как вектор. Следует разобраться, почему такое восприятие <<законно>>. Пусть $J$ --- матрица Якоби замены координат, тогда мы имеем формулы для векторов $\widetilde{\vec{\xi}} = J\vec{\xi}$ и для ковекторов: $\widetilde{\vec{\eta}} = (J^t)^{-1}\vec{\eta}$. Таким образом, если $J^{-1} = J^t$ (матрица $J$ ортогональна), то векторы и ковекторы преобразуются одинаково. Поэтому в случае ортонормированных координат в евклидовом пространстве мы можем не различать верхние и нижние индексы.
		\item Линейные операторы, действующие на векторах, задаются матрицей из элементов с одним нижним и одним верхним индексом: $\vec{\xi} = \A\vec{\eta}$, $\xi^i = a^i_j\eta^j$. При заменах координат они преобразуются, как тензоры типа $(1, 1)$:
			\[
				\widetilde{a}^i_j = \frac{\partial v^i}{\partial u^k}\frac{\partial u^l}{\partial v^j}a^k_l.
			\]
		\item Упомянем также квадратичные формы $\B(\vec{\xi}, \vec{\eta}) = b_{ij}\xi^i\eta^j$, которые являются тензорами типа $(0, 2)$:
			\[
				\widetilde{b}_{ij} = \frac{\partial v^k}{\partial u^i}\frac{\partial v^l}{\partial u^j}b_{kl}.
			\]
	\end{enumerate}
\end{example}

По любой паре, состоящей из верхнего и нижнего индексов, можно провести \textit{свёртку тензора}. Результат этой операции называется \textit{следом}. Она определяется следующим образом:

\begin{definition}
	Пусть фиксированы тензор типа $(p, q)$ $T = T^{i_1, \ldots, i_p}_{j_1, \ldots, j_q}$ и пара индексов $(i_k, j_l)$. Тензор $\tr T$ типа $(p - 1, q - 1)$ определяется формулой
	\[
		(\tr T)^{i_1, \ldots, i_p}_{j_1, \ldots, j_q} \vcentcolon = T^{i_1, \ldots, i_{k - 1}, i, i_{k + 1}, \ldots, i_p}_{j_1, \ldots, j_{l - 1}, i, j_{l + 1}, \ldots, j_q}
	\]
	и называется \textit{свёрткой} или \textit{следом} тензора $T$.
\end{definition}

\begin{definition}
	При наличии скалярного произведения, заданного матрицей $\G = (g_{ij})$, можно определить операцию \textit{опускания индекса}. Она сопоставляем тензору $T^{i_1, \ldots, i_p}_{j_1, \ldots, j_q}$ типа $(p, q)$ тензор $\widehat{T}^{i_2, \ldots, i_p}_{i_1, j_1, \ldots, j_q}$ типа $(p - 1, q + 1)$ по следующему правилу:
	\[
		\widehat{T}^{i_2, \ldots, i_p}_{i_1, j_1, \ldots, j_q} \vcentcolon = g_{i_1k}T^{k, i_2, \ldots, i_p}_{j_1, \ldots, j_q}.
	\]
	Аналогично определяется \textit{поднятие индекса}:
	\[
		\widehat{T}^{j_1, i_1,\ldots, i_p}_{j_2, \ldots, j_q} \vcentcolon = g^{j_1k}T^{i_1, \ldots, i_p}_{k, j_2, \ldots, j_q},
	\]
	где $g^{kl}$ --- матрица, обратная к $g_{ij}$.
\end{definition}

Легко видеть, что если мы сначала опустим индекс, а затем поднимем (или наоборот), то получим исходный тензор.

\subsection{Дифференциальные уравнения}

\begin{theorem}[О существовании и единственности]
	Решение дифференциального уравнения $\dot{\vec{x}} = \vec{v}(\vec{x}, t)$ с начальным условием $\vec{x}(t_0) = \vec{x}_0$ из области гладкости правой части существует и единственно (в том смысле, что всякие два решения с общим начальным условием совпадают в некоторой окрестности точки $t_0$).
\end{theorem}

\begin{theorem}[О гладкой зависимости от параметра]
	Значение в момент $t$ решения дифференциального уравнения $\dot{\vec{x}} = \vec{v}(\vec{x}, t; \mu)$ гладко зависит от параметра $\mu$, пробегающего некоторую область $M \subset \R^m$.
\end{theorem}

Помимо классической теоремы о существовании и единственности решения нам понадобится похожее утверждение для систем дифференциальных уравнений с двумя параметрами:
\begin{equation} \label{eq:DiffSystem}
	\begin{cases}
		\begin{aligned}
			\frac{\partial\vec{x}}{\partial u^1} = \vec{f}_1(\vec{x}, u^1, u^2),\\
			\frac{\partial\vec{x}}{\partial u^2} = \vec{f}_2(\vec{x}, u^1, u^2),
		\end{aligned}
	\end{cases}
\end{equation}
где $\vec{x} = (x^1, \ldots, x^n)$ --- неизвестная функция от $u^1$, $u^2$ со значениями в $\R^n$, а $\vec{f}_i = (\vec{f}_i^1, \ldots, \vec{f}_i^n)$, $i = 1, 2$, --- известные гладкие функции, определённые в некоторой открытой области $\Omega$ фазового пространства $\R^n \times \R \times \R \cong \R^{n + 2}$.

\begin{definition}
	Система \eqref{eq:DiffSystem} \textit{совместна}, если для любой тройки $(\vec{x}_0, u^1_0, u^2_0) \in \Omega$ она имеет гладкое решение $\vec{x}(u^1, u^2)$ с начальным условием $\vec{x}(u^1_0, u^2_0) = \vec{x}_0$, определённое в некоторой окрестности точки $(u^1_0, u^2_0)$.
\end{definition}

\begin{theorem}[Дарбу] \label{theorem:Darboux}
	Система \eqref{eq:DiffSystem} совместна тогда и только тогда, когда функции $\vec{f}_1$, $\vec{f}_2$ удовлетворяет следующему условию всюду в $\Omega$:
	\begin{equation} \label{eq:Darboux}
		\frac{\partial \vec{f}_1}{\partial x^i}f^i_2 + \frac{\partial\vec{f}_1}{\partial u^2} = \frac{\partial\vec{f}_2}{\partial x^j}f^j_1 + \frac{\partial\vec{f}_2}{\partial u^1}.
	\end{equation}
\end{theorem}

\begin{proof}
	$\Rightarrow$. Пусть $\vec{x}(u^1, u^2)$ --- гладкое решение системы с начальным условием $(u^1_0, u^2_0, \vec{x}_0)$. Тогда по теореме о дифференцировании сложной функции имеем
	\begin{multline*}
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \frac{\partial\vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}{\partial u^2} = \frac{\partial\vec{f}_1}{\partial x^i}(\vec{x}(u^1, u^2), u^1, u^2)\frac{\partial x^i}{\partial u^2} + {}\\{} + \frac{\partial\vec{f}_1}{\partial u^2}(\vec{x}(u^1, u^2), u^1, u^2) = \br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2).
	\end{multline*}
	Отсюда
	\[
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2).
	\]
	Аналогично получаем
	\[
		\br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2) = \frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \br{\frac{\partial\vec{f}_2}{\partial x^j}f_1^j + \frac{\partial\vec{f}_2}{\partial u^1}}(\vec{x}(u^1, u^2), u^1, u^2).
	\]

	$\Leftarrow$. Можем рассмотреть первое уравнение данной системы как обыкновенное дифференциальное уравнение по переменной $u^1$ с параметром $u^2$. Из теоремы о существовании и единственности следует, что при каждом значении $u^2 = u^2_0$ существует функция $\vec{y}(u^1)$, определённая в окрестности точки $u^1_0$ такая, что
	\[
		\frac{\partial\vec{y}}{\partial u^1} = \vec{f}_1(\vec{y}(u^1), u^1, u^2_0),\quad\vec{y}(u^1_0) = \vec{x}_0.
	\]

	Теперь для каждого $u^1$, для которого определено $\vec{y}(u^1)$, мы можем решить второе уравнение с начальным условием $\vec{x}(u^1, u_0^2) = \vec{y}(u^1)$. По той же теореме о существовании и единственности, а также по теореме о гладкой зависимости решения от параметра, полученное решение $\vec{x}(u^1, u^2)$ будет определено для всех $u^2$ из достаточно малой, не зависящей от $u^1$, окрестности точки $u^2_0$, и будет гладкой функцией от $u^1$, $u^2$.

	Итак, в малой окрестности точки $(u^1_0, u^2_0)$ мы построили гладкую функцию $\vec{x}(u^1, u^2)$, удовлетворяющую второму уравнению всюду, а первому --- во всех точках вида $(u^1, u^2_0)$, а также удовлетворяющую начальному условию $\vec{x}(u^1_0, u^2_0) = \vec{x}_0$. Осталось показать, что $\vec{x}(u^1, u^2)$ и первому уравнению удовлетворяет всюду, а не только вдоль прямой $u^2 = u^2_0$.

	Функция $\vec{x}(u^1, u^2)$ удовлетворяет второму уравнению на всей окрестности, поэтому можем, как и выше, написать
	\[
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\frac{\partial x^j}{\partial u^1} + \frac{\partial\vec{f}_2}{\partial u^1}(\vec{x}(u^1, u^2), u^1, u^2).
	\]
	(Только теперь мы ничего не знаем про $\partial x^j / \partial u^1$.) Используем условие \eqref{eq:Darboux}:
	\[
		\frac{\partial^2\vec{x}}{\partial u^1\partial u^2} = \underline{\frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\frac{\partial x^j}{\partial u^1}} + \br{\frac{\partial\vec{f}_1}{\partial x^i}f_2^i + \frac{\partial\vec{f}_1}{\partial u^2} - \underline{\frac{\partial\vec{f}_2}{\partial x^j}f_1^j}}(\vec{x}(u^1, u^2), u^1, u^2).
	\]
	Далее можем сгруппировать подчёркнутые суммы в одну:
	\begin{multline*}
		\frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\br{\frac{\partial x^j}{\partial u^1} - f_1^j(\vec{x}(u^1, u^2), u^1, u^2)} = \frac{\partial^2\vec{x}}{\partial u^1\partial u^2} - {}\\{} - \br{\frac{\partial\vec{f}_1}{\partial x^i}\underline{f_2^i} + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2) = \left\{\vec{f}_2(\vec{x}(u^1, u^2), u^1, u^2) = \frac{\partial\vec{x}}{\partial u^2}\right\} =\\ = \frac{\partial^2\vec{x}}{\partial u^1\partial u^2} - {\underbrace{\br{\frac{\partial\vec{f}_1}{\partial x^i}\frac{\partial x^i}{\partial u^2} + \frac{\partial\vec{f}_1}{\partial u^2}}(\vec{x}(u^1, u^2), u^1, u^2)}_{\frac{\scriptstyle\partial\vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}{\scriptstyle\partial u^2}}} = \frac{\partial}{\partial u^2}\br{\frac{\partial\vec{x}}{\partial u^1} - \vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}.
	\end{multline*}
	В итоге мы получаем:
	\[
		\frac{\partial}{\partial u^2}\underbrace{\br{\frac{\partial\vec{x}}{\partial u^1} - \vec{f}_1(\vec{x}(u^1, u^2), u^1, u^2)}}_{{} \vcentcolon = \vec{g}(u^1, u^2)} = \frac{\partial\vec{f}_2}{\partial x^j}(\vec{x}(u^1, u^2), u^1, u^2)\underbrace{\br{\frac{\partial x^j}{\partial u^1} - f_1^j(\vec{x}(u^1, u^2), u^1, u^2)}}_{g^j(u^1, u^2)},
	\]
	это дифференциальное уравнение на функцию $\vec{g}$. Его можно рассмотреть как обыкновенное дифференциальное уравнение по $u^2$ с параметром $u^1$. При этом для всех $u^1$ выполнено начальное условие $\vec{g}(u^1, u^2_0) = \vec{0}$, откуда $\vec{g}(u^1, u^2) \equiv \vec{0}$. Это и означает, что $\vec{x}(u^1, u^2)$ всюду удовлетворяет первому уравнению системы.
\end{proof}

Отметим, что условия \eqref{eq:Darboux} не нужно запоминать. Они являются просто записью, очевидно, необходимого условия
\[
	\frac{\partial\vec{x}}{\partial u^1 \partial u^2} = \frac{\partial\vec{x}}{\partial u^2 \partial u^1}.
\]
Нетривиальное утверждение теоремы заключается в том, что это условие оказывается не только необходимым, но и достаточным.

\begin{example} \label{example:SimpleDiffJointness}
	Если правые части системы \eqref{eq:DiffSystem} не зависят от $\vec{x}$:
	\[
		\begin{cases}
			\begin{aligned}
				&\frac{\partial\vec{x}}{\partial u^1} = \vec{f}_1(u^1, u^2),\\
				&\frac{\partial\vec{x}}{\partial u^2} = \vec{f}_2(u^1, u^2),
			\end{aligned}
		\end{cases}
	\]
	то условие совместности для них выглядит следующим образом:
	\[
		\frac{\partial\vec{f}_1}{\partial u^2} = \frac{\partial\vec{f}_2}{\partial u^1}.
	\]
\end{example}

\begin{theorem}[О продолжении] \label{theorem:ContinuityDifferential}
	Решение дифференциального уравнения $\dot{\vec{x}} = \vec{v}(\vec{x}, t)$ с начальным условием из компакта в фазовом пространстве можно продолжить в обе стороны либо неограниченно, либо до границы этого компакта.
\end{theorem}

\subsection{Про функции в геометрии}

Фразу, упомянутую в эпиграфе к данному разделу, А.\,В. Пенской произнёс на первой лекции своего курса по дифференциальной геометрии и относилась она к записи вида
\[
	\vec{r}(t) = \vec{r}(s(t)),
\]
где под $\vec{r}$ понимается параметризация некоторой кривой, а $t$ и $s$ --- два разных (гладких) параметра на ней.

С точки зрения анализа эта запись, конечно же, некорректна, но мы хотим так писать, ведь она хорошо выражает смысл происходящего --- при смене параметризации сама кривая не меняется. На самом деле, наше интуитивное желание можно выразить и формально.

\shorthandoff{"}%
\[\begin{tikzcd}
	{\R_t} \\
	& {\mathbb{E}} && {\mathbb{R}^n} \\
	{\R_s}
	\arrow["\Phi", from=1-1, to=2-2]
	\arrow["\vec{r}", from=2-2, to=2-4]
	\arrow["\Psi"', from=3-1, to=2-2]
	\arrow["\homeo" left, "\zeta" right, from=1-1, to=3-1]
\end{tikzcd}\]
\shorthandon{"}%

Имеет место такая коммутативная диаграмма, где отображения $\Phi$ и $\Psi$ задают выбор параметра (соответственно, $t$ или $s$), а $\zeta$ --- диффеоморфизм, выражающий смену параметра. Запись $\vec{r}(t) = \vec{r}(s(t))$ мы будем понимать не буквально, а следующим образом:
\[
	\vec{r} \circ \Phi = \vec{r} \circ \Psi \circ \zeta.
\]

В курсе мы будем довольно часто пользоваться такой записью, так как она чрезвычайно удобна для выражения кривой в разных параметризациях. Это короткое отступление было необходимо, чтобы обосновать корректность наших выкладок и устранить путаницу.

